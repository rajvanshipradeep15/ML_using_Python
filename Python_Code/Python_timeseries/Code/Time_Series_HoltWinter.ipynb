{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Forecast - Exponential Smoothing\n",
    "\n",
    "### Kumar Rahul\n",
    "\n",
    "Forecasting the demand of services or products leads to better management of short term or long term planning. In this case, we are looking at the warranty related issues reported, on a particular brand of two-wheeler. The data is a monthly roll-up of approximately half a million issues reported by the customers over a four year period. \n",
    "We will be using Claim forecasting data in this exercise. Refer the **Exhibit 1** to understand the feature list. Use the data and answer the below questions.\n",
    "\n",
    "1.\tLoad the time series dataset in Jupyter Notebook using pandas.\n",
    "2.\tSplit the data into training set and test set. Use walk forward validation strategy for model building and evaluation.\n",
    "3.\tGiven recent claim, what is the expected claim for the next time period? Build a model with statsmodel.api to forecast the amount claimed in next time step.\n",
    "4.\t How do you interpret the model outcome? Report the model performance on the walk forward validation set.\n",
    "\n",
    "**Exhibit 1**\n",
    "\n",
    "|Sl. No.|Name of Variable|Variable Description|\n",
    "|----------|------------|---------------|\n",
    "|1\t|date\t|Date of Claim|\n",
    "|2\t|rate\t|Amount claimed|\n",
    "|3\t|item\t|Number of claims|\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and clean-up data\n",
    "from numpy import nan\n",
    "from numpy import isnan\n",
    "from pandas import to_numeric\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from math import sqrt\n",
    "from numpy import split\n",
    "from numpy import array\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from matplotlib import pyplot\n",
    "#import matplotlib.pylab as plt\n",
    "\n",
    "import statsmodels.tsa.holtwinters as hw\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing, SimpleExpSmoothing, Holt\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "monthly_raw_df = pd.read_csv('./data/data_monthly.csv', sep=',', header=0, infer_datetime_format=True, \n",
    "                             index_col=['date'], \n",
    "                             parse_dates= ['date'],dayfirst=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_raw_df.sort_index(inplace=True)\n",
    "monthly_raw_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data for the first day and last day of CGM monitoring being trucated as it has not been captured for the full cycle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_filter_df = monthly_raw_df.filter(['rate'], axis =1)\n",
    "monthly_filter_df['rate'] = monthly_filter_df['rate'].map(lambda x:str(x).replace(',', '')).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_filter_df = monthly_filter_df[(monthly_filter_df.index >='2014-03-01') & \n",
    "                                      (monthly_filter_df.index <= '2017-05-31')]\n",
    "\n",
    "monthly_filter_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Framing\n",
    "\n",
    "\n",
    "We will use the data to explore a very specific question; that is:\n",
    "\n",
    "**Given recent claim, what is the expected claim for the next time period?**\n",
    "\n",
    "Plot of the original data is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pyplot.figure(figsize = (18, 5))\n",
    "pyplot.plot(monthly_filter_df, 'b-')\n",
    "pyplot.title('Monthly amount claimed over a 3 year period')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test Sets\n",
    "We will use the first three years of data for training predictive models and the final year for evaluating models.\n",
    "\n",
    "The function split_filter_df() below splits the monthly data into train and test sets and organizes each into standard weeks.\n",
    "\n",
    "Specific row offsets are used to split the data using knowledge of the filter_df. The split filter_dfs are then organized into  data using the NumPy split() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_filter_df(data):\n",
    "    split_point = len(data) - 10\n",
    "    train, test = data[0:split_point], data[split_point:]\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the new file\n",
    "train, test = split_filter_df(monthly_filter_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate train data\n",
    "print(train.shape)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate test data\n",
    "print(test.shape)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metric\n",
    "\n",
    "Both Root Mean Squared Error (RMSE) and Mean Absolute Error (MAE) can be used. Unlike MAE, RMSE is more punishing of forecast errors.\n",
    "\n",
    "The function evaluate_forecasts_rmse() and evaluate_forecasts_mape() is being used for evaluating model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate one or more  forecasts against expected values\n",
    "def evaluate_forecasts_rmse(actual):\n",
    "    score_rmse = 0\n",
    "    se = 0\n",
    "    # calculate an RMSE score for each day\n",
    "    for i in range(actual.shape[0]):\n",
    "        # calculate mse\n",
    "        se += (actual.iloc[i,0] - actual.iloc[i,1])**2\n",
    "        # calculate rmse\n",
    "    score_rmse = sqrt(se/actual.shape[0])\n",
    "    return score_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate one or more  forecasts against expected values\n",
    "def evaluate_forecasts_mape(actual):\n",
    "    score_mape = 0\n",
    "    ape = 0\n",
    "    for i in range(actual.shape[0]):\n",
    "        # calculate mse\n",
    "        ape += np.abs(((actual.iloc[i,0] - actual.iloc[i,1])/actual.iloc[i,0]))\n",
    "        # calculate mape\n",
    "    score_mape = (ape)/actual.shape[0]\n",
    "    return actual, score_mape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Develop Basic Model\n",
    "\n",
    "The model developed here is ExponentialSmooting (TES).\n",
    "\n",
    "* SES - SimpleExpSmoothing\n",
    "* Holt - DES\n",
    "* ExponentialSmoothing - TES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Walk-Forward validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate a single model\n",
    "def evaluate_model(model_func, train, test,alpha,beta,gamma,season):\n",
    "    # history is a list of  data\n",
    "    history = train.filter(['rate'], axis = 1)\n",
    "    #print(history)\n",
    "    # walk-forward validation over each week\n",
    "    predictions = list()\n",
    "    for i in range(len(test)):\n",
    "        # predict the week\n",
    "        yhat_sequence = model_func(history.values, alpha, beta, gamma, season)\n",
    "        # store the predictions\n",
    "        predictions.append(yhat_sequence)      \n",
    "        # get real observation and add to history for predicting the next week\n",
    "        history = history.append(test.iloc[[i]])\n",
    "    predictions = array(predictions)\n",
    "    test['prediction'] = predictions\n",
    "    # evaluate predictions days for each week\n",
    "    actual, score_mape = evaluate_forecasts_mape(test[:])\n",
    "    score_rmse = evaluate_forecasts_rmse(test[:])\n",
    "    return actual, score_mape, score_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forecast\n",
    "def exp_forecast(history, alpha, beta, gamma, season):\n",
    "    # define the model\n",
    "    model = ExponentialSmoothing(history, seasonal = season, seasonal_periods=12)\n",
    "    # fit the model\n",
    "    model_fit = model.fit(smoothing_level=alpha, smoothing_slope=beta, smoothing_seasonal = gamma, optimized=True)\n",
    "    # make forecast\n",
    "    yhat = model_fit.forecast(steps=1)[0]\n",
    "    return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_column(test):\n",
    "    for n in test.columns:\n",
    "        if n =='prediction':\n",
    "            test.drop('prediction', axis = 1, inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# define the names and functions for the models which is to be evaluated\n",
    "models = dict()\n",
    "models['ExponentialSmoothening'] = exp_forecast\n",
    "\n",
    "import itertools\n",
    "alpha = [0.2,0.4,0.6, 0.8]\n",
    "beta  = [0.4,0.5, 0.6]\n",
    "gamma = [0.1, 0.5]\n",
    "season = ['add','mul']\n",
    "\n",
    "grid_values = list(itertools.product(alpha, beta, gamma,season))\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# evaluate each model\n",
    "for name, func in models.items():\n",
    "    best_mape,best_rmse, best_alpha, best_beta, best_gamma, best_season = float(\"inf\"), float(\"inf\"),float(\"inf\"),float(\"inf\"),float(\"inf\"), str()\n",
    "    \n",
    "    for i in range(0, len(grid_values)):\n",
    "        alpha = grid_values[i][0]\n",
    "        beta = grid_values[i][1]\n",
    "        gamma = grid_values[i][2]\n",
    "        season = grid_values[i][3]\n",
    "        del_column(test)\n",
    "        try:\n",
    "            actual, score_mape, score_rmse= evaluate_model(func, train, test,alpha, beta, gamma,season)\n",
    "            if score_rmse < best_rmse:\n",
    "                best_rmse, best_mape, best_alpha, best_beta, best_gamma, best_season = score_rmse, score_mape, alpha, beta, gamma,season\n",
    "            print('alpha = %.2f beta = %.2f gamma = %.2f MAPE=%.5f RMSE=%.4f Model=%s' % (alpha, beta, gamma,score_mape,score_rmse, season))\n",
    "        except:\n",
    "            continue\n",
    "    print('Best alpha = %.2f beta = %.2f gamma = %.2f MAPE=%.5f RMSE=%.5f Model = %s' % (best_alpha, best_beta, best_gamma,best_mape, best_rmse, season))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the best parameter to run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = dict()\n",
    "models['ExponentialSmoothening'] = exp_forecast\n",
    "\n",
    "# evaluate each model\n",
    "for name, func in models.items():\n",
    "    alpha = 0.8\n",
    "    beta = 0.4\n",
    "    gamma = 0.1\n",
    "    season = 'mul'\n",
    "    del_column(test)\n",
    "    actual, score_mape, score_rmse= evaluate_model(func, train, test,alpha, beta, gamma,season)\n",
    "    print('MAPE=%.5f RMSE=%.5f' % (best_mape, best_rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#actual.to_csv('holts_winter_monthly_forecast.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "1. Trend is also a parameter in `ExponentialSmoothing` model. However, we have not defined that in grid_values.  Modify the code to include trend in the grid search for ExponentialSmoothing Model.\n",
    "2. Develop SES and Holt (DES) model by modifying the code in this notebook. \n",
    "3. Compare the MAPE for SES, DES and TES model. Which model will you go ahead with?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
