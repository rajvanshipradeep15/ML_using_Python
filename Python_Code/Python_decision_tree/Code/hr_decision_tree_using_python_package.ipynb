{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family: Arial; font-size:2.75em;color:purple; font-style:bold\"><br>\n",
    "Decision Tree using Python (sklearn):</p><br>\n",
    "<p style=\"font-family: Arial; font-size:2.25em;color:green; font-style:bold\"><br>\n",
    "Kumar Rahul</p><br>\n",
    "\n",
    "### We will be using HR data in this exercise. Refer the Exhibit 1 to understand the feature list. Use the HR data and answer the below questions.\n",
    "\n",
    "Load the dataset in Jupyter Notebook using pandas\n",
    "Create a new data frame with the numeric features and categorical features as dummy variable coded features. Which features will you include for model building and why?\n",
    "Split the data into training set and test set. Use 80% of data for model training and 20% for model testing.\n",
    "> 1. Building the Decision Tree model and understand the dummy variable coding while working on DT models\n",
    "2. Visualize the decision tree and intrepret the decision tree business rules\n",
    "3. Validate the outcome of the model on test set and report precision, recall, F-score on test set\n",
    "4. Understand the concept of pipeline\n",
    "\n",
    "**Some advantages of decision trees are:**\n",
    "\n",
    "> 1. Simple to understand and to interpret. Trees can be visualised.\n",
    "2. Requires little data preparation. Other techniques often require data normalisation, dummy variables need to be created and blank values to be removed. Note however that the sklearn decision tree module does not support missing values.\n",
    "3. Able to handle both numerical and categorical data. Other techniques are usually specialised in analysing datasets that have only one type of variable. See algorithms for more information.\n",
    "4. Uses a white box model. If a given situation is observable in a model, the explanation for the condition is easily explained by boolean logic.\n",
    "\n",
    "\n",
    "**The disadvantages of decision trees include:**\n",
    "\n",
    "> 1. Decision-tree learners can create over-complex trees that do not generalise the data well. This is called overfitting. Mechanisms such as pruning (not currently supported), setting the minimum number of samples required at a leaf node or setting the maximum depth of the tree are necessary to avoid this problem.\n",
    "2. Decision trees can be unstable because small variations in the data might result in a completely different tree being generated. This problem is mitigated by using decision trees within an ensemble.\n",
    "3. Decision tree learners create biased trees if some classes dominate. It is therefore recommended to balance the dataset prior to fitting with the decision tree.\n",
    "\n",
    "**Exhibit 1**\n",
    "\n",
    "\n",
    "|Sl.No.|Name of Variable|Variable Description|\n",
    "|:-------|----------------|:--------------------|\n",
    "|1\t|Candidate reference number|\tUnique number to identify the candidate|\n",
    "|2\t|DOJ extended|Binary variable identifying whether candidate asked for date of joining extension (Yes/No)|\n",
    "|3\t|Duration to accept the offer|\tNumber of days taken by the candidate to accept the offer (continuous variable)|\n",
    "|4\t|Notice period|\tNotice period to be served in the parting company before candidate can join this company (continuous variable)|\n",
    "|5\t|Offered band|\tBand offered to the candidate based on experience and performance in interview rounds (categorical variable labelled C0/C1/C2/C3/C4/C5/C6)|\n",
    "|6\t|Percentage hike (CTC) expected|\tPercentage hike expected by the candidate (continuous variable)|\n",
    "|7\t|Percentage hike offered (CTC)| Percentage hike offered by the company (continuous variable)|\n",
    "|8\t|Percent difference CTC|\tPercentage difference between offered and expected CTC (continuous variable)|\n",
    "|9\t|Joining bonus|\tBinary variable indicating if joining bonus was given or not (Yes/No)|\n",
    "|10\t|Gender|\tGender of the candidate (Male/Female)|\n",
    "|11\t|Candidate source|\tSource from which resume of the candidate was obtained (categorical variables with categories  Employee referral/Agency/Direct)|\n",
    "|12\t|REX (in years)|\tRelevant years of experience of the candidate for the position offered (continuous variable)|\n",
    "|13\t|LOB|\tLine of business for which offer was rolled out (categorical variable)|\n",
    "|14\t|DOB|\tDate of birth of the candidate|\n",
    "|15\t|Joining location|\tCompany location for which offer was rolled out for candidate to join (categorical variable)|\n",
    "|16\t|Candidate relocation status|\tBinary variable indicating whether candidate has to relocate from one city to another city for joining (Yes/No)|\n",
    "|17 |HR status|\tFinal joining status of candidate (Joined/Not-Joined)|\n",
    "\n",
    "***\n",
    "\n",
    "# Code starts here\n",
    "\n",
    "To know the environment with the python kernal\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use below mentioned libraries for **data import, processing and visulization**. As we progress, we will use other specific libraries for model building and evaluation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Rmd_chunk_options": "libraries, echo=TRUE, message=FALSE, warning=FALSE",
    "autoscroll": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import seaborn as sn # visualization library based on matplotlib\n",
    "import matplotlib.pylab as plt\n",
    "import graphviz \n",
    "\n",
    "#the output of plotting commands is displayed inline within frontends like in Jupyter notebook\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Data Import and Manipulation\n",
    "\n",
    "### 1. Importing a data set\n",
    "\n",
    "_Give the correct path to the data_\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "modify the ast_note_interactivity kernel option to see the value of multiple statements at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Rmd_chunk_options": "readData, echo=TRUE,tidy=TRUE",
    "autoscroll": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "raw_df = pd.read_csv( \"../HR_case/data/IMB533_HR_Data_No_Missing_Value.csv\", \n",
    "                        sep = ',', na_values = ['', ' '])\n",
    "\n",
    "raw_df.columns = raw_df.columns.str.lower().str.replace(' ', '_')\n",
    "raw_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping SLNo and Candidate.Ref as these will not be used for any analysis or model building. To know about all the possible operations which can be performed on pandas dataframe: \n",
    "\n",
    "https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if set(['slno','candidate_ref']).issubset(raw_df.columns):\n",
    "    raw_df.drop(['slno','candidate_ref'],axis=1, inplace=True)\n",
    "    \n",
    "raw_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2. Structure of the dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Rmd_chunk_options": "summarizeData, echo=TRUE,tidy=TRUE",
    "autoscroll": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "raw_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df.status.value_counts()\n",
    "raw_df.describe(include='all').transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a help on the features of a object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#?raw_df.status.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Summarizing the dataset\n",
    "Create a new data frame and store the raw data copy. This is being done to have a copy of the raw data intact for further manipulation if needed. The *dropna()* function is used for row wise deletion of missing value. The axis = 0 means row-wise, 1 means column wise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Rmd_chunk_options": "createDataCopy, echo=TRUE,tidy=TRUE",
    "autoscroll": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "filter_df = raw_df.dropna(axis=0, how='any', thresh=None, \n",
    "                             subset=None, inplace=False)\n",
    "\n",
    "list(filter_df.columns )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first start by printing the unique labels in categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "numerical_features = ['duration_to_accept_offer','notice_period','pecent_hike_expected_in_ctc',\n",
    "                      'percent_hike_offered_in_ctc','percent_difference_ctc','rex_in_yrs','age']\n",
    "\n",
    "categorical_features = ['doj_extended','offered_band','joining_bonus','candidate_relocate_actual',\n",
    "                        'gender','candidate_source','lob','location','status']\n",
    "\n",
    "for f in categorical_features:\n",
    "    print(\"\\nThe unique labels in {} is {}\\n\".format(f, filter_df[f].unique()))\n",
    "    print(\"The values in {} is \\n{}\\n\".format(f,  filter_df[f].value_counts()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the feature **line of business** it seems that *EAS, Healthcare and MMS* does not have enough observations and may be clubbed together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_df['lob']=np.where(filter_df['lob'] =='EAS', 'Others', filter_df['lob'])\n",
    "filter_df['lob']=np.where(filter_df['lob'] =='Healthcare', 'Others', filter_df['lob'])\n",
    "filter_df['lob']=np.where(filter_df['lob'] =='MMS', 'Others', filter_df['lob'])\n",
    "filter_df.lob.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use **groupby** function of pandas to get deeper insights of the behaviour of people **Joining** or **Not Joining** the company. We will write a generic function to report the mean by any categorical variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_by (categorical_features):\n",
    "    return filter_df.groupby(categorical_features).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_by(\"doj_extended\")\n",
    "group_by(\"status\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Visualizing the Data\n",
    "\n",
    "Plot can be done using the callable functions of \n",
    "\n",
    ">1. pandas library (http://pandas.pydata.org/pandas-docs/stable/visualization.html)\n",
    "2. matplotlib library (https://matplotlib.org/) or\n",
    "3. seaborn library (https://seaborn.pydata.org/) which is based on matplotlib and provides interface for drawing attractive statistical graphics.\n",
    "\n",
    "#### 3a. Visualizing the Data using seaborn\n",
    "\n",
    "Write a custom function to create bar plot to visualize the average of numeric features w.r.t each categorical feature. Say, average number of days to accept the offer w.r.t status as joined vs. not joined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bar_plot(xlabel,ylabel,xcnt,ycnt):\n",
    "    sn.barplot(x = xlabel, y = ylabel, data= filter_df, ax = axes[xcnt,ycnt])\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "numerical_features_set = ['duration_to_accept_offer','notice_period','age']\n",
    "categorical_features_set = ['offered_band','gender','status']\n",
    "\n",
    "xcnt=0\n",
    "ycnt = 0\n",
    "fig, axes = plt.subplots(3,3, figsize=(12,9))\n",
    "fig.subplots_adjust(hspace = 1, wspace=.5)\n",
    "\n",
    "for c in categorical_features_set:\n",
    "    for n in numerical_features_set:\n",
    "        bar_plot(c,n,xcnt,ycnt)\n",
    "        ycnt = ycnt+1\n",
    "    xcnt = xcnt+1\n",
    "    ycnt=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Dummy Variable coding\n",
    "\n",
    "Remove the response variable from the dataset¶\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_features = list(filter_df.columns)\n",
    "X_features.remove('status')\n",
    "X_features.remove('pecent_hike_expected_in_ctc')\n",
    "X_features.remove('percent_hike_offered_in_ctc')\n",
    "X_features.remove('candidate_relocate_actual')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful read on the dummy variable code while working on decision tree classifier:\n",
    "\n",
    "> https://datascience.stackexchange.com/questions/47638/in-which-cases-shouldnt-we-drop-the-first-level-of-categorical-variables\n",
    "> https://towardsdatascience.com/understanding-decision-tree-classification-with-scikit-learn-2ddf272731bd\n",
    "> https://datascience.stackexchange.com/questions/5226/strings-as-features-in-decision-tree-random-forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_X_df = pd.get_dummies(filter_df[X_features])\n",
    "encoded_Y_df = pd.get_dummies(filter_df['status'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Demo of use of preprocessing\n",
    "\n",
    "Using Label Encoder. But not to be used in DecisionTree Classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn import preprocessing\n",
    "#le = preprocessing.LabelEncoder()\n",
    "#for i in range(0,filter_df.shape[1]):\n",
    "#    if filter_df.dtypes[i]=='object':\n",
    "#        filter_df[filter_df.columns[i]] = le.fit_transform(filter_df[filter_df.columns[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = encoded_Y_df.filter(['Joined'], axis =1)\n",
    "X = encoded_X_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train and test split can also be done using the **sklearn module**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, Y, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building: Using the **sklearn** \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.DecisionTreeClassifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Rmd_chunk_options": "caretModel, echo=TRUE, message=FALSE, warning=FALSE",
    "autoscroll": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "dt = tree.DecisionTreeClassifier()\n",
    "\n",
    "dt_model = dt.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Issue with graphviz, you may refer to solutions here: https://stackoverflow.com/questions/28312534/graphvizs-executables-are-not-found-python-3-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install six\n",
    "#!conda install graphviz\n",
    "#!pip install pydotplus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.externals.six import StringIO\n",
    "from six import StringIO\n",
    "from IPython.display import Image\n",
    "from sklearn import tree\n",
    "import pydotplus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = StringIO()\n",
    "vis_tree = tree.export_graphviz(dt_model,out_file=output_file,\n",
    "                                feature_names=X_train.columns,  \n",
    "                                class_names=['Not Joined','Joined'],  \n",
    "                                filled=True, rounded=True,  \n",
    "                                special_characters=True) \n",
    "graph = pydotplus.graph_from_dot_data(output_file.getvalue())\n",
    "graph.write_png(\"hr_decision_tree.png\")\n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other way to visulaize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import graphviz \n",
    "\n",
    "vis_tree = tree.export_graphviz(dt_model,out_file=None,\n",
    "                                feature_names=X_train.columns,  \n",
    "                                class_names=['Not Joined','Joined'],  \n",
    "                                filled=True, rounded=True,  \n",
    "                                special_characters=True) \n",
    "\n",
    "graph = graphviz.Source(vis_tree) \n",
    "graph.render(\"hr_decision_tree\") \n",
    "graph.view\n",
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. The prediction on test data.\n",
    "\n",
    "The prediction can be carried out by **defining functions** as well. Below is one such instance wherein a function is defined and is used for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Rmd_chunk_options": "caretPrediction, echo=TRUE",
    "autoscroll": false
   },
   "outputs": [],
   "source": [
    "def get_predictions ( test_class, model, test_data ):\n",
    "    y_pred_df = pd.DataFrame( { 'actual_class': test_class,\n",
    "                               'predicted_value': dt_model.predict(test_data)})\n",
    "    return y_pred_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Giving label to the Y column of the test set by using the dictionary data type in python. This is being done for the model which was built using dummy variable coding. It will be used to generate confusion matrix at a later time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ser = y_test\n",
    "status_dict = {1:\"Joined\", 0:\"Not Joined\"}\n",
    "y_test_df = ser.replace(dict(Joined=status_dict))\n",
    "y_test_df.rename({'Joined': 'actual'}, axis='columns', inplace=True )\n",
    "y_test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_model_df = pd.DataFrame(get_predictions(y_test_df.actual, dt_model, X_test))\n",
    "dt_model_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_model_df['predicted_class'] = dt_model_df.predicted_value.map(lambda x: 'Joined' if x >= 1 else 'Not Joined')\n",
    "dt_model_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Confusion Matrix\n",
    "\n",
    "We will built classification matrix using the **metrics** method from **sklearn** package. We will also write a custom function to build a classification matrix and use it for reporting the performance measures.\n",
    "\n",
    "#### 3a. Confusion Matrix using sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"The dt model with dummy variable coding output: \")\n",
    "\n",
    "confusion_matrix(dt_model_df.actual_class, dt_model_df.predicted_class)\n",
    "\n",
    "dt_report = (classification_report(dt_model_df.actual_class, dt_model_df.predicted_class))\n",
    "print(dt_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3b. Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Precision for Joined Class is %.2f \" %(1123/(1123+186)))\n",
    "print(\"Recall for Not Joined Class is %.2f \" %(154/(336+154)))\n",
    "print(\"Macro Average Recall is %.3f\" % np.divide((0.86 + 0.31),2))\n",
    "print(\"Weighted Recall is %.2f\" % (0.86*(1459/(1459+340))+0.31*(340/(1459+340))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3c. Recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Recall for Joined Class is %.2f \" %(1123/(1123+336)))\n",
    "print(\"Recall for Not Joined Class is %.2f \" %(154/(154+186)))\n",
    "print(\"Macro Average Recall is %.2f\" % np.divide((0.77 + 0.45),2))\n",
    "print(\"Weighted Recall is %.2f\" % (0.77*(1459/(1459+340))+0.45*(340/(1459+340))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3b Confusion Matrix using generic function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_cm( actual, predicted ):\n",
    "    plt.figure(figsize=(9,9))\n",
    "    cm = metrics.confusion_matrix( actual, predicted )\n",
    "    sn.heatmap(cm, annot=True,  fmt='.0f', xticklabels = [\"Joined\", \"Not Joined\"] , \n",
    "               yticklabels = [\"Joined\", \"Not Joined\"],cmap = 'Blues_r')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.title('Classification Matrix Plot', size = 15);\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classification matrix plot as reported by **model 1**  with dummy variable coding is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_cm( dt_model_df.actual_class, dt_model_df.predicted_class )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Performance Measure on the test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_performance (clasf_matrix):\n",
    "    measure = pd.DataFrame({\n",
    "                        'sensitivity': [round(clasf_matrix[0,0]/(clasf_matrix[0,0]+clasf_matrix[0,1]),2)], \n",
    "                        'specificity': [round(clasf_matrix[1,1]/(clasf_matrix[1,0]+clasf_matrix[1,1]),2)],\n",
    "                        'recall': [round(clasf_matrix[0,0]/(clasf_matrix[0,0]+clasf_matrix[0,1]),2)],\n",
    "                        'precision': [round(clasf_matrix[0,0]/(clasf_matrix[0,0]+clasf_matrix[1,0]),2)],\n",
    "                        'overall_acc': [round((clasf_matrix[0,0]+clasf_matrix[1,1])/\n",
    "                                              (clasf_matrix[0,0]+clasf_matrix[0,1]+clasf_matrix[1,0]+clasf_matrix[1,1]),2)]\n",
    "                       })\n",
    "    return measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = metrics.confusion_matrix(dt_model_df.actual_class, dt_model_df.predicted_class)\n",
    "\n",
    "dt_model_metrics_df = pd.DataFrame(measure_performance(cm))\n",
    "dt_model_metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage of Pipelines\n",
    "\n",
    "\n",
    "Pipelines can be used to perform a sequence of steps before model building. It expects a sequence to be passed as list of tuples. Sequentially apply a list of transforms and a final estimator. Intermediate steps of the pipeline must be ‘transforms’, that is, they must implement fit and transform methods. The final estimator only needs to implement fit. \n",
    "\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "seq_steps = [('dt', tree.DecisionTreeClassifier())]\n",
    "pipeline = Pipeline(seq_steps)\n",
    "print(pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model with Grid Search\n",
    "\n",
    "To report the performance on the selected KPI use `sklearn.metrics.SCORERS.keys()` to get the list of all the metrics and pass the relevant one in `RandomizedSearchCV` or `GridSearchCV`\n",
    "\n",
    "Implement Grid Search to fine tune the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = ['gini','entropy']     #2\n",
    "max_features = [None, 'auto', 'log2','sqrt'] #4\n",
    "max_depth = [2,3,4] #3 \n",
    "min_samples_split = [50,75,100,120] #4\n",
    "min_samples_leaf = [50, 75] #2\n",
    "class_weight = ['balanced',None] #2\n",
    "\n",
    "# Create the grid\n",
    "random_grid = {'dt__criterion': criterion,\n",
    "               'dt__max_features' :  max_features,\n",
    "               'dt__max_depth' :  max_depth,\n",
    "               'dt__min_samples_split': min_samples_split,\n",
    "               'dt__min_samples_leaf' : min_samples_leaf,\n",
    "               'dt__class_weight' : class_weight}\n",
    "random_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import SCORERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCORERS.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Use the random grid to search for best hyperparameters\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#tree_model = tree.DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Random search of parameters, using 3,4 and 5 fold cross validation, \n",
    "\n",
    "for cv in tqdm(range(3,6)):\n",
    "    best_tree_model = GridSearchCV(estimator = pipeline, param_grid = random_grid, \n",
    "                                   scoring = \"balanced_accuracy\", cv = cv)\n",
    "    # Fit the random search model\n",
    "    best_tree_model.fit(X_train, y_train.values.ravel())\n",
    "    print(\"performance for %d fold CV = %2.2f\" %(cv, best_tree_model.score(X_test,y_test)))\n",
    "    print(\"best parameters for %d fold CV\" %(cv))\n",
    "    print(best_tree_model.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_tree_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_tree_model_df = pd.DataFrame(get_predictions(y_test_df.actual, best_tree_model, X_test))\n",
    "best_tree_model_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_tree_model_df['predicted_class'] = best_tree_model_df.predicted_value.map(lambda x: 'Joined' \n",
    "                                                                               if x >= 1 else 'Not Joined')\n",
    "best_tree_model_df[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_cm( best_tree_model_df.actual_class, best_tree_model_df.predicted_class )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = metrics.confusion_matrix(best_tree_model_df.actual_class, best_tree_model_df.predicted_class)\n",
    "\n",
    "best_dt_model_metrics_df = pd.DataFrame(measure_performance(cm))\n",
    "best_dt_model_metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "1. Implement RandomizedSearchCV using model selection module.\n",
    "2. Compare the outcome of RandomizedSearchCV with GridSearchCV and document your finding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** \n",
    "End of Document\n"
   ]
  }
 ],
 "metadata": {
  "Rmd_header": {
   "author": "Kumar Rahul",
   "date": "9 September 2016",
   "output": "word_document",
   "title": "Logistic Regression using Caret Package"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
