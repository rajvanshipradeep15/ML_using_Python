{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family: Arial; font-size:2.75em;color:purple; font-style:bold\"><br>\n",
    "Ridge Regression using Python (sklearn):</p><br>\n",
    "<p style=\"font-family: Arial; font-size:2.25em;color:green; font-style:bold\"><br>\n",
    "Kumar Rahul</p><br>\n",
    "\n",
    "### We will be using DAD hospital data in this exercise. Refer the Exhibit 1 to understand the feature list. Use the DAD Hospital data and answer the below questions.\n",
    "\n",
    "1.\tLoad the dataset in Jupyter Notebook using pandas\n",
    "2.\tBuild a correlation matrix between all the numeric features in the dataset. Report the features, which are correlated at a cut-off of 0.70. What actions will you take on the features, which are highly correlated?\n",
    "3.\tBuild a new feature named BMI using body height and body weight. Include this as a part of the data frame created in step 1.\n",
    "4.\tPast medical history code has 175 instances of missing value (NaN). Impute ‘None’ as a label wherever the value is NaN for this feature.\n",
    "5.\tCreate a new data frame with the numeric features and categorical features as dummy variable coded features. Which features will you include for model building and why?\n",
    "6.\tSplit the data into training set and test set. Use 80% of data for model training and 20% for model testing. \n",
    "7.\tBuild a model using age as independent variable and cost of treatment as dependent variable.\n",
    "    > * Is age a significant feature in this model?\n",
    "    * What inferences can be drawn from this model? \n",
    "8.\tBuild a model with statsmodel.api to estimate the total cost to hospital. How do you interpret the model outcome? Report the model performance on the test set.\n",
    "9.\tBuild a model with statsmodel.formula.api to estimate the total cost to hospital and report the model performance on the test set. What difference do you observe in the model built here and the one built in step 8.\n",
    "10.\tBuild a model using sklearn package to estimate the total cost to hospital. What difference do you observe in this model compared to model built in step 8 and 9.\n",
    "11. Build a model using lasso, ridge and elastic net regression. What differences do you observe?\n",
    "12. Build model using gradient descent to get an intuition about the inner working of optimization algorithms.\n",
    "13. Build model using gradient descent with regularization to get an intution about the inner working of optimization algorithms.\n",
    "\n",
    "**PS: Not all the questions are being answered as a part of the same notebook. You are encouraged to answer the questions if you find them missing.**\n",
    "\n",
    "**Exhibit 1**\n",
    "\n",
    "|Sl.No.|Variable|\tDescription|\n",
    "|------|--------|--------------|\n",
    "|1|Age|\t Age of the patient in years|\n",
    "|2|Body Weight|\t Weight of the patient in Kilograms|\n",
    "|3|Body Height| \tHeight of the patient in cm|\n",
    "|4|HR Pulse|\t Pulse of patient at the time of admission|\n",
    "|5|BP-High|\t High BP of patient (Systolic)|\n",
    "|6|BP-Low|\t Low BP of patient (Diastolic)|\n",
    "|7|RR|\t Respiratory rate of patient|\n",
    "|8|HB|\t Hemoglobin count of patient|\n",
    "|9|Urea|\t Urea levels of patient|\n",
    "|10|Creatinine|\t Creatinine levels of patient|\n",
    "|11|Marital Status|\t Marital status of the patient|\n",
    "|12|Gender|\t  Gender code for patient|\n",
    "|13|Past Medical History Code|\t Code given to the past medical history of the Patient|\n",
    "|14|Mode of Arrival|\t Way in which the patient arrived the hospital|\n",
    "|15|State at the Time of Arrival|\t State in which the patient arrived|\n",
    "|16|Type of Admission|\t Type of admission for the patient|\n",
    "|17|Key Complaints Code|\t Codes given to the key complaints faced by the patient|\n",
    "|18|Total Cost to Hospital|\t Actual cost incurred by the hospital|\n",
    "|19|Total Length of Stay|\t Number of days patient stayed in the hospital|\n",
    "|20|Length of Stay - ICU|\t Number of days patient stayed in the ICU|\n",
    "|21|Length of Stay - Ward|\t Number of days patient stayed in the ward|\n",
    "|22|Implant used (Y/N)|\t Any implant done on the patient|\n",
    "|23|Cost of Implant|\t Total cost of all the implants done on the patient, if any|\n",
    "\n",
    "\n",
    "***\n",
    "\n",
    "# Code starts here\n",
    "\n",
    "To know the environment with the pyhton kernal\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "\n",
    "sys.executable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppress the warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use below mentioned libraries for **data import, processing and visulization**. As we progress, we will use other specific libraries for model building and evaluation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Rmd_chunk_options": "libraries, echo=TRUE, message=FALSE, warning=FALSE",
    "autoscroll": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import seaborn as sn # visualization library based on matplotlib\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "#the output of plotting commands is displayed inline within Jupyter notebook\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Data Import and Manipulation\n",
    "\n",
    "### 1. Importing a data set\n",
    "\n",
    "_Give the correct path to the data_\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "modify the ast_note_interactivity kernel option to see the value of multiple statements at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the display settings for columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = None\n",
    "#pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas will start looking from where your current python file is located. Therefore you can move from your current directory to where your data is located with '..'\n",
    "\n",
    "> * The single period . means current working directory\n",
    "* The double period .. means parent of the current working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Rmd_chunk_options": "readData, echo=TRUE,tidy=TRUE",
    "autoscroll": false
   },
   "outputs": [],
   "source": [
    "raw_df = pd.read_csv( \"<give the pathname>\", \n",
    "                        sep = ',', na_values = ['', ' '])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Rmd_chunk_options": "readData, echo=TRUE,tidy=TRUE",
    "autoscroll": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "raw_df.columns = raw_df.columns.str.lower().str.replace('.', '_')\n",
    "raw_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping SL No as these will not be used for any analysis or model building."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#?raw_df.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if set(['sl no']).issubset(raw_df.columns):\n",
    "    #write your code\n",
    "    \n",
    "raw_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2. Structure of the dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Rmd_chunk_options": "summarizeData, echo=TRUE,tidy=TRUE",
    "autoscroll": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "raw_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Give Statistical summary of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get numeric features from the data and find the corelation amongst numeric features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "numerical_features = [x for x in raw_df.select_dtypes(include=[np.number])]\n",
    "numerical_features\n",
    "\n",
    "numerical_features_df = raw_df.select_dtypes(include=[np.number])\n",
    "numerical_features_df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = [x for x in raw_df.select_dtypes(include=[np.object])]\n",
    "categorical_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Summarizing the dataset\n",
    "Create a new data frame and store the raw data copy. This is being done to have a copy of the raw data intact for further manipulation if needed. The *dropna()* function is used for row wise deletion of missing value. The axis = 0 means row-wise, 1 means column wise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Rmd_chunk_options": "createDataCopy, echo=TRUE,tidy=TRUE",
    "autoscroll": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "filter_df = raw_df.dropna(axis=0, how='any', thresh=None, \n",
    "                             subset=None, inplace=False)\n",
    "\n",
    "list(filter_df.columns )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first start by printing the unique labels in categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#write your code using `unique` and `value_counts` method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clubbing some of the feature labels together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_df['past_medical_history_code']=np.where(\n",
    "        (filter_df['past_medical_history_code'] =='hypertension1') |\n",
    "         (filter_df['past_medical_history_code'] =='hypertension2') | \n",
    "         (filter_df['past_medical_history_code'] =='hypertension3'),\n",
    "    'hypertension', filter_df['past_medical_history_code'])\n",
    "\n",
    "filter_df['past_medical_history_code']=np.where(\n",
    "    (filter_df['past_medical_history_code'] =='Diabetes1') |\n",
    "    (filter_df['past_medical_history_code'] =='Diabetes2'), \n",
    "    'diabetes', filter_df['past_medical_history_code'])\n",
    "\n",
    "\n",
    "filter_df['key_complaints__code']=np.where(\n",
    "        (filter_df['key_complaints__code'] =='other- respiratory') |\n",
    "         (filter_df['key_complaints__code'] =='PM-VSD') | \n",
    "         (filter_df['key_complaints__code'] =='CAD-SVD') |\n",
    "        (filter_df['key_complaints__code'] =='CAD-VSD') |\n",
    "        (filter_df['key_complaints__code'] =='other-nervous') |\n",
    "        (filter_df['key_complaints__code'] =='other-general'), \n",
    "        'others', filter_df['key_complaints__code'])\n",
    "\n",
    "#filter_df.past_medical_history_code.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use **groupby** function of pandas to get deeper insights of the behaviour of people **Joining** or **Not Joining** the company. We will write a generic function to report the mean by any categorical variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_by (categorical_features):\n",
    "    return filter_df.groupby(categorical_features).mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_by(\"past_medical_history_code\")\n",
    "group_by(\"key_complaints__code\")\n",
    "group_by(\"marital_status\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating BMI\n",
    "\n",
    "\\begin{equation}\n",
    " BMI = \\frac{Body Weight}{Body Height_{mtr}^2}\n",
    "\\end{equation} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write your code to compute bmi here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Visualizing the Data\n",
    "\n",
    "Plot can be done using the callable functions of \n",
    "\n",
    ">1. pandas library (http://pandas.pydata.org/pandas-docs/stable/visualization.html)\n",
    "2. matplotlib library (https://matplotlib.org/) or\n",
    "3. seaborn library (https://seaborn.pydata.org/) which is based on matplotlib and provides interface for drawing attractive statistical graphics.\n",
    "\n",
    "#### 3a. Visualizing the Data using seaborn\n",
    "\n",
    "Write a custom function to create bar plot to visualize the average of numeric features w.r.t each categorical feature. Say, average age w.r.t gender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write your custom function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Call the function to plot the bar charts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model using sklearn:\n",
    "\n",
    "### Dummy Variable coding\n",
    "\n",
    "Remove the response variable from the dataset¶\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_features = list(filter_df.columns)\n",
    "\n",
    "X_features = [x for x in filter_df if x not in ['body_weight','body_height',\n",
    "                                           'creatinine','state_at_the_time_of_arrival',\n",
    "                                           'total_amount_billed_to_the_patient','concession',\n",
    "                                          'actual_receivable_amount','total_length_of_stay',\n",
    "                                          'length_of_stay___icu','length_of_stay__ward',\n",
    "                                            'total_cost_to_hospital']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = ['gender','marital_status','key_complaints__code',\n",
    "                        'past_medical_history_code','mode_of_arrival','type_of_admsn','implant_used']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_X_df = pd.get_dummies(filter_df[X_features], columns = categorical_features, drop_first = True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = None\n",
    "encoded_X_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = filter_df.filter(['total_cost_to_hospital'], axis =1)\n",
    "X = encoded_X_df\n",
    "Y.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and test data split using Python\n",
    "\n",
    "The train and test split can also be done using the **sklearn module**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = #write your code to split the data in 80:20 ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building: Using the **sklearn** \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To know about all the methods inside a library use the `dir` command after invoking the library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "\n",
    "#dir(linear_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Rmd_chunk_options": "caretModel, echo=TRUE, message=FALSE, warning=FALSE",
    "autoscroll": false
   },
   "outputs": [],
   "source": [
    "# Create linear regression object\n",
    "ridge_reg_model = linear_model.Ridge(alpha = 0.5) #alpha = 0 is same as simple regression with OLS\n",
    "\n",
    "# Train the model using the training sets\n",
    "ridge_reg_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making the model is as simple as calling the `fit` method for `Ridge`. However, since we would like to select the best value of alpha, lets try to do it using the below function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions using the testing set\n",
    "y_pred = ridge_reg_model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The coefficients\n",
    "print('Coefficients: \\n', ridge_reg_model.coef_)\n",
    "print('Intercept: \\n', ridge_reg_model.intercept_)\n",
    "\n",
    "# The mean squared error\n",
    "print(\"Mean squared error: %.2f\"\n",
    "      % mean_squared_error(y_train, y_pred))\n",
    "\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print('Variance score: %.2f' % r2_score(y_train, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Search with cross validation\n",
    "\n",
    "To use RandomizedSearchCV, create a parameter grid from where sample will be picked during model building:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of trees in random forest\n",
    "alpha = [x for x in np.arange(0.0,5.5,.5)]\n",
    "\n",
    "# Create the grid\n",
    "random_grid = {'alpha': alpha}\n",
    "random_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model with Grid Search\n",
    "\n",
    "To report the performance on the selected KPI use `sklearn.metrics.SCORERS.keys()` to get the list of all the metrics and pass the relevant one in `RandomizedSearchCV` or `GridSearchCV`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import SCORERS\n",
    "\n",
    "SCORERS.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the random grid to search for best hyperparameters\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "ridge_reg_model = linear_model.Ridge()\n",
    "\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "ridge_best_model = GridSearchCV(estimator = ridge_reg_model, \n",
    "                               param_grid = random_grid, scoring = \"r2\",\n",
    "                               cv = 3, verbose=0)\n",
    "# Fit the random search model\n",
    "ridge_best_model.fit(X_train, y_train.values.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report the parameter\n",
    "\n",
    "The best model has the following parameter selected from the random search grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_best_model.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. The prediction on test data.\n",
    "\n",
    "The prediction can be carried out by **defining functions** as well. Below is one such instance wherein a function is defined and is used for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Rmd_chunk_options": "caretPrediction, echo=TRUE",
    "autoscroll": false
   },
   "outputs": [],
   "source": [
    "def get_predictions ( test_actual, model, test_data ):\n",
    "    df = pd.DataFrame(model.predict(test_data))\n",
    "    df.columns = ['predicted']\n",
    "    y_pred_df = pd.concat([test_actual.reset_index(drop=True), df], axis = 1)\n",
    "    return y_pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_test_df = pd.DataFrame(get_predictions(y_test.total_cost_to_hospital, ridge_best_model, X_test))\n",
    "predict_test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(predict_test_df.total_cost_to_hospital,predict_test_df.predicted)\n",
    "mean_squared_error(predict_test_df.total_cost_to_hospital,predict_test_df.predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment - Save model\n",
    "\n",
    "Two ways to save the model.\n",
    "\n",
    "* Using joblib\n",
    "* Using pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump( ridge_best_model, \"ridge_best_model.joblib\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write your code to save the model using pickle. Explore `pickle.dumps` or `pickle.dump`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use model on New Cases\n",
    "\n",
    "We can load the model object for later use. Assuming that X_test is a new data on which we will want to use the model. We can load the model object in two different ways:\n",
    "\n",
    "* Using joblib\n",
    "* Using pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "model_joblib = joblib.load( \"ridge_best_model.joblib\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_joblib.predict( X_test )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model performance on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_joblib.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write your code to load the model using pickle and predict on the test set. Explore `pickle.load` or `pickle.predict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### End of Document\n",
    "\n",
    "***\n",
    "***\n"
   ]
  }
 ],
 "metadata": {
  "Rmd_header": {
   "author": "Kumar Rahul",
   "date": "9 September 2016",
   "output": "word_document",
   "title": "Logistic Regression using Caret Package"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
